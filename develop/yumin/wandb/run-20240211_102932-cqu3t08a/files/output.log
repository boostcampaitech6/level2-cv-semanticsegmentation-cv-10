/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
2024-02-11 10:30:39 | Epoch [1/150], Step [25/156], Loss: 0.0146
2024-02-11 10:31:25 | Epoch [1/150], Step [50/156], Loss: 0.0121
2024-02-11 10:32:12 | Epoch [1/150], Step [75/156], Loss: 0.0091
2024-02-11 10:32:59 | Epoch [1/150], Step [100/156], Loss: 0.0055
2024-02-11 10:33:45 | Epoch [1/150], Step [125/156], Loss: 0.0041
2024-02-11 10:34:32 | Epoch [1/150], Step [150/156], Loss: 0.0031
2024-02-11 10:35:34 | Epoch [2/150], Step [25/156], Loss: 0.0026
2024-02-11 10:36:20 | Epoch [2/150], Step [50/156], Loss: 0.0021
2024-02-11 10:37:07 | Epoch [2/150], Step [75/156], Loss: 0.0021
2024-02-11 10:37:54 | Epoch [2/150], Step [100/156], Loss: 0.002
2024-02-11 10:38:40 | Epoch [2/150], Step [125/156], Loss: 0.002
2024-02-11 10:39:27 | Epoch [2/150], Step [150/156], Loss: 0.0019
2024-02-11 10:40:29 | Epoch [3/150], Step [25/156], Loss: 0.0013
2024-02-11 10:41:16 | Epoch [3/150], Step [50/156], Loss: 0.0013
2024-02-11 10:42:02 | Epoch [3/150], Step [75/156], Loss: 0.0012
2024-02-11 10:42:49 | Epoch [3/150], Step [100/156], Loss: 0.0012
2024-02-11 10:43:36 | Epoch [3/150], Step [125/156], Loss: 0.0011
2024-02-11 10:44:23 | Epoch [3/150], Step [150/156], Loss: 0.0013
2024-02-11 10:45:25 | Epoch [4/150], Step [25/156], Loss: 0.0011
2024-02-11 10:46:11 | Epoch [4/150], Step [50/156], Loss: 0.0009
2024-02-11 10:46:58 | Epoch [4/150], Step [75/156], Loss: 0.0012
2024-02-11 10:47:45 | Epoch [4/150], Step [100/156], Loss: 0.0009
2024-02-11 10:48:32 | Epoch [4/150], Step [125/156], Loss: 0.0011
2024-02-11 10:49:18 | Epoch [4/150], Step [150/156], Loss: 0.001
2024-02-11 10:50:21 | Epoch [5/150], Step [25/156], Loss: 0.0009
2024-02-11 10:51:08 | Epoch [5/150], Step [50/156], Loss: 0.0008
2024-02-11 10:51:54 | Epoch [5/150], Step [75/156], Loss: 0.001
2024-02-11 10:52:41 | Epoch [5/150], Step [100/156], Loss: 0.001
2024-02-11 10:53:28 | Epoch [5/150], Step [125/156], Loss: 0.0011
2024-02-11 10:54:14 | Epoch [5/150], Step [150/156], Loss: 0.001
Start validation # 5














































































 99%|███████████████████████████████████████████████████████████████████████████████████████████▊ | 78/79 [06:14<00:04,  4.82s/it]
current valid Dice: 0.2689
Best performance at epoch: 5, 0.0000 -> 0.2689

100%|█████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [06:19<00:00,  4.81s/it]
2024-02-11 11:01:37 | Epoch [6/150], Step [25/156], Loss: 0.0011
2024-02-11 11:02:24 | Epoch [6/150], Step [50/156], Loss: 0.001
2024-02-11 11:03:11 | Epoch [6/150], Step [75/156], Loss: 0.001
2024-02-11 11:03:57 | Epoch [6/150], Step [100/156], Loss: 0.0009
2024-02-11 11:04:44 | Epoch [6/150], Step [125/156], Loss: 0.0012
2024-02-11 11:05:30 | Epoch [6/150], Step [150/156], Loss: 0.001
2024-02-11 11:06:33 | Epoch [7/150], Step [25/156], Loss: 0.001
2024-02-11 11:07:19 | Epoch [7/150], Step [50/156], Loss: 0.0011
2024-02-11 11:08:06 | Epoch [7/150], Step [75/156], Loss: 0.0013
2024-02-11 11:08:53 | Epoch [7/150], Step [100/156], Loss: 0.0021
2024-02-11 11:09:39 | Epoch [7/150], Step [125/156], Loss: 0.0012
2024-02-11 11:10:26 | Epoch [7/150], Step [150/156], Loss: 0.0013
2024-02-11 11:11:28 | Epoch [8/150], Step [25/156], Loss: 0.0017
2024-02-11 11:12:15 | Epoch [8/150], Step [50/156], Loss: 0.0015
2024-02-11 11:13:02 | Epoch [8/150], Step [75/156], Loss: 0.0011
2024-02-11 11:13:48 | Epoch [8/150], Step [100/156], Loss: 0.0011
2024-02-11 11:14:35 | Epoch [8/150], Step [125/156], Loss: 0.001
2024-02-11 11:15:22 | Epoch [8/150], Step [150/156], Loss: 0.0017
2024-02-11 11:16:24 | Epoch [9/150], Step [25/156], Loss: 0.0015
2024-02-11 11:17:10 | Epoch [9/150], Step [50/156], Loss: 0.0013
2024-02-11 11:17:57 | Epoch [9/150], Step [75/156], Loss: 0.001
2024-02-11 11:18:44 | Epoch [9/150], Step [100/156], Loss: 0.001
2024-02-11 11:19:30 | Epoch [9/150], Step [125/156], Loss: 0.0011
2024-02-11 11:20:17 | Epoch [9/150], Step [150/156], Loss: 0.0009
2024-02-11 11:21:19 | Epoch [10/150], Step [25/156], Loss: 0.0008
2024-02-11 11:22:06 | Epoch [10/150], Step [50/156], Loss: 0.0009
2024-02-11 11:22:52 | Epoch [10/150], Step [75/156], Loss: 0.0013
2024-02-11 11:23:39 | Epoch [10/150], Step [100/156], Loss: 0.0016
2024-02-11 11:24:26 | Epoch [10/150], Step [125/156], Loss: 0.0014
2024-02-11 11:25:12 | Epoch [10/150], Step [150/156], Loss: 0.0009
Start validation #10














































































 99%|███████████████████████████████████████████████████████████████████████████████████████████▊ | 78/79 [06:30<00:04,  4.89s/it]

100%|█████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [06:35<00:00,  5.01s/it]
2024-02-11 11:32:51 | Epoch [11/150], Step [25/156], Loss: 0.0084
2024-02-11 11:33:37 | Epoch [11/150], Step [50/156], Loss: 0.0014
2024-02-11 11:34:24 | Epoch [11/150], Step [75/156], Loss: 0.0009
2024-02-11 11:35:11 | Epoch [11/150], Step [100/156], Loss: 0.0008
2024-02-11 11:35:58 | Epoch [11/150], Step [125/156], Loss: 0.0008
2024-02-11 11:36:44 | Epoch [11/150], Step [150/156], Loss: 0.001
2024-02-11 11:37:46 | Epoch [12/150], Step [25/156], Loss: 0.0007
2024-02-11 11:38:33 | Epoch [12/150], Step [50/156], Loss: 0.0008
2024-02-11 11:39:20 | Epoch [12/150], Step [75/156], Loss: 0.0007
2024-02-11 11:40:06 | Epoch [12/150], Step [100/156], Loss: 0.0008
2024-02-11 11:40:53 | Epoch [12/150], Step [125/156], Loss: 0.0008
2024-02-11 11:41:40 | Epoch [12/150], Step [150/156], Loss: 0.0007
2024-02-11 11:42:42 | Epoch [13/150], Step [25/156], Loss: 0.0009
2024-02-11 11:43:29 | Epoch [13/150], Step [50/156], Loss: 0.0007
Traceback (most recent call last):
  File "/data/ephemeral/home/level2-cv-semanticsegmentation-cv-10/develop/yumin/custom.py", line 385, in <module>
    set_seed()
  File "/data/ephemeral/home/level2-cv-semanticsegmentation-cv-10/develop/yumin/custom.py", line 326, in train
    scaler.step(optimizer)
  File "/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 416, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 314, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 314, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt