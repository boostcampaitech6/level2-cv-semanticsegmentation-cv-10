/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
2024-02-09 22:54:59 | Epoch [1/150], Step [25/80], Loss: 0.637
2024-02-09 22:55:29 | Epoch [1/150], Step [50/80], Loss: 0.5713
2024-02-09 22:55:58 | Epoch [1/150], Step [75/80], Loss: 0.5084
2024-02-09 22:56:44 | Epoch [2/150], Step [25/80], Loss: 0.4544
2024-02-09 22:57:18 | Epoch [2/150], Step [50/80], Loss: 0.4135
2024-02-09 22:57:49 | Epoch [2/150], Step [75/80], Loss: 0.3767
2024-02-09 22:58:35 | Epoch [3/150], Step [25/80], Loss: 0.35
2024-02-09 22:59:06 | Epoch [3/150], Step [50/80], Loss: 0.3332
2024-02-09 22:59:36 | Epoch [3/150], Step [75/80], Loss: 0.3165
2024-02-09 23:00:23 | Epoch [4/150], Step [25/80], Loss: 0.3085
2024-02-09 23:00:55 | Epoch [4/150], Step [50/80], Loss: 0.3052
2024-02-09 23:01:25 | Epoch [4/150], Step [75/80], Loss: 0.3006
2024-02-09 23:02:13 | Epoch [5/150], Step [25/80], Loss: 0.2994
2024-02-09 23:02:43 | Epoch [5/150], Step [50/80], Loss: 0.2981
2024-02-09 23:03:14 | Epoch [5/150], Step [75/80], Loss: 0.3009
2024-02-09 23:03:59 | Epoch [6/150], Step [25/80], Loss: 0.2936
2024-02-09 23:04:30 | Epoch [6/150], Step [50/80], Loss: 0.2901
2024-02-09 23:05:00 | Epoch [6/150], Step [75/80], Loss: 0.2858
2024-02-09 23:05:47 | Epoch [7/150], Step [25/80], Loss: 0.2792
2024-02-09 23:06:16 | Epoch [7/150], Step [50/80], Loss: 0.2568
2024-02-09 23:06:47 | Epoch [7/150], Step [75/80], Loss: 0.2446
2024-02-09 23:07:34 | Epoch [8/150], Step [25/80], Loss: 0.2225
2024-02-09 23:08:05 | Epoch [8/150], Step [50/80], Loss: 0.2019
2024-02-09 23:08:33 | Epoch [8/150], Step [75/80], Loss: 0.1876
2024-02-09 23:09:19 | Epoch [9/150], Step [25/80], Loss: 0.1654
2024-02-09 23:09:52 | Epoch [9/150], Step [50/80], Loss: 0.1474
2024-02-09 23:10:21 | Epoch [9/150], Step [75/80], Loss: 0.1343
2024-02-09 23:11:08 | Epoch [10/150], Step [25/80], Loss: 0.1192
2024-02-09 23:11:40 | Epoch [10/150], Step [50/80], Loss: 0.108
2024-02-09 23:12:11 | Epoch [10/150], Step [75/80], Loss: 0.0999
  0%|                                                                     | 0/40 [00:00<?, ?it/s]







































 98%|██████████████████████████████████████████████████████████▌ | 39/40 [05:21<00:08,  8.23s/it]
current valid Dice: 0.0037
Best performance at epoch: 10, 0.0000 -> 0.0037

100%|████████████████████████████████████████████████████████████| 40/40 [05:29<00:00,  8.24s/it]
2024-02-09 23:18:27 | Epoch [11/150], Step [25/80], Loss: 0.0921
2024-02-09 23:18:55 | Epoch [11/150], Step [50/80], Loss: 0.0861
2024-02-09 23:19:23 | Epoch [11/150], Step [75/80], Loss: 0.0802
2024-02-09 23:20:06 | Epoch [12/150], Step [25/80], Loss: 0.0763
2024-02-09 23:20:34 | Epoch [12/150], Step [50/80], Loss: 0.0708
2024-02-09 23:21:02 | Epoch [12/150], Step [75/80], Loss: 0.0694
2024-02-09 23:21:46 | Epoch [13/150], Step [25/80], Loss: 0.067
2024-02-09 23:22:14 | Epoch [13/150], Step [50/80], Loss: 0.0644
2024-02-09 23:22:42 | Epoch [13/150], Step [75/80], Loss: 0.0647
2024-02-09 23:23:27 | Epoch [14/150], Step [25/80], Loss: 0.0633
2024-02-09 23:23:56 | Epoch [14/150], Step [50/80], Loss: 0.0639
2024-02-09 23:24:23 | Epoch [14/150], Step [75/80], Loss: 0.0636
2024-02-09 23:25:07 | Epoch [15/150], Step [25/80], Loss: 0.0632
2024-02-09 23:25:36 | Epoch [15/150], Step [50/80], Loss: 0.0615
2024-02-09 23:26:04 | Epoch [15/150], Step [75/80], Loss: 0.0622
2024-02-09 23:26:47 | Epoch [16/150], Step [25/80], Loss: 0.0623
2024-02-09 23:27:16 | Epoch [16/150], Step [50/80], Loss: 0.0616
2024-02-09 23:27:44 | Epoch [16/150], Step [75/80], Loss: 0.0609
2024-02-09 23:28:28 | Epoch [17/150], Step [25/80], Loss: 0.0596
2024-02-09 23:28:56 | Epoch [17/150], Step [50/80], Loss: 0.0585
2024-02-09 23:29:24 | Epoch [17/150], Step [75/80], Loss: 0.0559
2024-02-09 23:30:08 | Epoch [18/150], Step [25/80], Loss: 0.0555
2024-02-09 23:30:36 | Epoch [18/150], Step [50/80], Loss: 0.0522
2024-02-09 23:31:04 | Epoch [18/150], Step [75/80], Loss: 0.0499
2024-02-09 23:31:49 | Epoch [19/150], Step [25/80], Loss: 0.048
2024-02-09 23:32:17 | Epoch [19/150], Step [50/80], Loss: 0.0443
2024-02-09 23:32:45 | Epoch [19/150], Step [75/80], Loss: 0.0417
2024-02-09 23:33:29 | Epoch [20/150], Step [25/80], Loss: 0.0399
2024-02-09 23:33:58 | Epoch [20/150], Step [50/80], Loss: 0.0366
2024-02-09 23:34:26 | Epoch [20/150], Step [75/80], Loss: 0.0348
  0%|                                                                     | 0/40 [00:00<?, ?it/s]







































 98%|██████████████████████████████████████████████████████████▌ | 39/40 [05:20<00:08,  8.32s/it]
current valid Dice: 0.1362
Best performance at epoch: 20, 0.0037 -> 0.1362

100%|████████████████████████████████████████████████████████████| 40/40 [05:28<00:00,  8.22s/it]
2024-02-09 23:40:40 | Epoch [21/150], Step [25/80], Loss: 0.0331
2024-02-09 23:41:08 | Epoch [21/150], Step [50/80], Loss: 0.0313
2024-02-09 23:41:36 | Epoch [21/150], Step [75/80], Loss: 0.0304
2024-02-09 23:42:19 | Epoch [22/150], Step [25/80], Loss: 0.0291
2024-02-09 23:42:48 | Epoch [22/150], Step [50/80], Loss: 0.0285
2024-02-09 23:43:15 | Epoch [22/150], Step [75/80], Loss: 0.0285
2024-02-09 23:43:59 | Epoch [23/150], Step [25/80], Loss: 0.0278
2024-02-09 23:44:27 | Epoch [23/150], Step [50/80], Loss: 0.027
2024-02-09 23:44:55 | Epoch [23/150], Step [75/80], Loss: 0.027
2024-02-09 23:45:39 | Epoch [24/150], Step [25/80], Loss: 0.0271
2024-02-09 23:46:07 | Epoch [24/150], Step [50/80], Loss: 0.0265
2024-02-09 23:46:36 | Epoch [24/150], Step [75/80], Loss: 0.0262
2024-02-09 23:47:19 | Epoch [25/150], Step [25/80], Loss: 0.0265
2024-02-09 23:47:47 | Epoch [25/150], Step [50/80], Loss: 0.0263
2024-02-09 23:48:15 | Epoch [25/150], Step [75/80], Loss: 0.0262
2024-02-09 23:48:58 | Epoch [26/150], Step [25/80], Loss: 0.0261
2024-02-09 23:49:27 | Epoch [26/150], Step [50/80], Loss: 0.0261
2024-02-09 23:49:55 | Epoch [26/150], Step [75/80], Loss: 0.0263
2024-02-09 23:50:37 | Epoch [27/150], Step [25/80], Loss: 0.0256
2024-02-09 23:51:06 | Epoch [27/150], Step [50/80], Loss: 0.0252
2024-02-09 23:51:34 | Epoch [27/150], Step [75/80], Loss: 0.025
2024-02-09 23:52:18 | Epoch [28/150], Step [25/80], Loss: 0.0239
2024-02-09 23:52:47 | Epoch [28/150], Step [50/80], Loss: 0.0241
2024-02-09 23:53:14 | Epoch [28/150], Step [75/80], Loss: 0.0228
2024-02-09 23:53:57 | Epoch [29/150], Step [25/80], Loss: 0.0226
Traceback (most recent call last):
  File "/data/ephemeral/home/level2-cv-semanticsegmentation-cv-10/develop/yumin/custom.py", line 352, in <module>
    train(model, train_loader, valid_loader, criterion, optimizer)
  File "/data/ephemeral/home/level2-cv-semanticsegmentation-cv-10/develop/yumin/custom.py", line 300, in train
    loss.backward()
  File "/opt/conda/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt