/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
2024-02-10 10:16:15 | Epoch [1/150], Step [25/320], Loss: 0.2113
2024-02-10 10:16:56 | Epoch [1/150], Step [50/320], Loss: 0.0816
2024-02-10 10:17:38 | Epoch [1/150], Step [75/320], Loss: 0.0486
2024-02-10 10:18:20 | Epoch [1/150], Step [100/320], Loss: 0.0343
2024-02-10 10:19:01 | Epoch [1/150], Step [125/320], Loss: 0.0295
2024-02-10 10:19:43 | Epoch [1/150], Step [150/320], Loss: 0.0266
2024-02-10 10:20:24 | Epoch [1/150], Step [175/320], Loss: 0.0248
2024-02-10 10:21:06 | Epoch [1/150], Step [200/320], Loss: 0.0262
2024-02-10 10:21:47 | Epoch [1/150], Step [225/320], Loss: 0.0252
2024-02-10 10:22:29 | Epoch [1/150], Step [250/320], Loss: 0.0204
2024-02-10 10:23:11 | Epoch [1/150], Step [275/320], Loss: 0.0238
2024-02-10 10:23:52 | Epoch [1/150], Step [300/320], Loss: 0.0186
2024-02-10 10:25:11 | Epoch [2/150], Step [25/320], Loss: 0.017
2024-02-10 10:25:52 | Epoch [2/150], Step [50/320], Loss: 0.0173
2024-02-10 10:26:34 | Epoch [2/150], Step [75/320], Loss: 0.0155
2024-02-10 10:27:16 | Epoch [2/150], Step [100/320], Loss: 0.0142
2024-02-10 10:27:57 | Epoch [2/150], Step [125/320], Loss: 0.0121
2024-02-10 10:28:39 | Epoch [2/150], Step [150/320], Loss: 0.0137
2024-02-10 10:29:20 | Epoch [2/150], Step [175/320], Loss: 0.011
2024-02-10 10:30:02 | Epoch [2/150], Step [200/320], Loss: 0.0098
2024-02-10 10:30:44 | Epoch [2/150], Step [225/320], Loss: 0.0118
2024-02-10 10:31:25 | Epoch [2/150], Step [250/320], Loss: 0.0105
2024-02-10 10:32:07 | Epoch [2/150], Step [275/320], Loss: 0.0107
2024-02-10 10:32:48 | Epoch [2/150], Step [300/320], Loss: 0.0115
2024-02-10 10:34:07 | Epoch [3/150], Step [25/320], Loss: 0.0084
2024-02-10 10:34:49 | Epoch [3/150], Step [50/320], Loss: 0.007
2024-02-10 10:35:31 | Epoch [3/150], Step [75/320], Loss: 0.0085
2024-02-10 10:36:12 | Epoch [3/150], Step [100/320], Loss: 0.0073
2024-02-10 10:36:54 | Epoch [3/150], Step [125/320], Loss: 0.0097
2024-02-10 10:37:35 | Epoch [3/150], Step [150/320], Loss: 0.0087
2024-02-10 10:38:17 | Epoch [3/150], Step [175/320], Loss: 0.0062
2024-02-10 10:38:59 | Epoch [3/150], Step [200/320], Loss: 0.0078
2024-02-10 10:39:40 | Epoch [3/150], Step [225/320], Loss: 0.006
2024-02-10 10:40:22 | Epoch [3/150], Step [250/320], Loss: 0.0073
2024-02-10 10:41:04 | Epoch [3/150], Step [275/320], Loss: 0.0065
2024-02-10 10:41:45 | Epoch [3/150], Step [300/320], Loss: 0.0074
2024-02-10 10:43:04 | Epoch [4/150], Step [25/320], Loss: 0.006
2024-02-10 10:43:45 | Epoch [4/150], Step [50/320], Loss: 0.0049
2024-02-10 10:44:27 | Epoch [4/150], Step [75/320], Loss: 0.0065
2024-02-10 10:45:08 | Epoch [4/150], Step [100/320], Loss: 0.005
2024-02-10 10:45:50 | Epoch [4/150], Step [125/320], Loss: 0.0049
2024-02-10 10:46:32 | Epoch [4/150], Step [150/320], Loss: 0.0115
2024-02-10 10:47:13 | Epoch [4/150], Step [175/320], Loss: 0.0046
2024-02-10 10:47:55 | Epoch [4/150], Step [200/320], Loss: 0.0061
2024-02-10 10:48:37 | Epoch [4/150], Step [225/320], Loss: 0.0044
2024-02-10 10:49:18 | Epoch [4/150], Step [250/320], Loss: 0.0045
2024-02-10 10:50:00 | Epoch [4/150], Step [275/320], Loss: 0.0045
2024-02-10 10:50:41 | Epoch [4/150], Step [300/320], Loss: 0.0043
2024-02-10 10:52:00 | Epoch [5/150], Step [25/320], Loss: 0.0055
2024-02-10 10:52:42 | Epoch [5/150], Step [50/320], Loss: 0.0052
2024-02-10 10:53:23 | Epoch [5/150], Step [75/320], Loss: 0.0071
2024-02-10 10:54:05 | Epoch [5/150], Step [100/320], Loss: 0.0041
2024-02-10 10:54:47 | Epoch [5/150], Step [125/320], Loss: 0.006
2024-02-10 10:55:28 | Epoch [5/150], Step [150/320], Loss: 0.0046
2024-02-10 10:56:10 | Epoch [5/150], Step [175/320], Loss: 0.0044
2024-02-10 10:56:52 | Epoch [5/150], Step [200/320], Loss: 0.0049
2024-02-10 10:57:33 | Epoch [5/150], Step [225/320], Loss: 0.0047
2024-02-10 10:58:15 | Epoch [5/150], Step [250/320], Loss: 0.0046
2024-02-10 10:58:56 | Epoch [5/150], Step [275/320], Loss: 0.0066
2024-02-10 10:59:38 | Epoch [5/150], Step [300/320], Loss: 0.0067
2024-02-10 11:00:57 | Epoch [6/150], Step [25/320], Loss: 0.006
2024-02-10 11:01:38 | Epoch [6/150], Step [50/320], Loss: 0.0045
2024-02-10 11:02:20 | Epoch [6/150], Step [75/320], Loss: 0.0044
2024-02-10 11:03:02 | Epoch [6/150], Step [100/320], Loss: 0.0045
2024-02-10 11:03:43 | Epoch [6/150], Step [125/320], Loss: 0.0047
2024-02-10 11:04:25 | Epoch [6/150], Step [150/320], Loss: 0.0046
2024-02-10 11:05:07 | Epoch [6/150], Step [175/320], Loss: 0.0041
2024-02-10 11:05:48 | Epoch [6/150], Step [200/320], Loss: 0.0042
2024-02-10 11:06:30 | Epoch [6/150], Step [225/320], Loss: 0.0038
2024-02-10 11:07:11 | Epoch [6/150], Step [250/320], Loss: 0.0048
2024-02-10 11:07:53 | Epoch [6/150], Step [275/320], Loss: 0.004
2024-02-10 11:08:35 | Epoch [6/150], Step [300/320], Loss: 0.0065
2024-02-10 11:09:54 | Epoch [7/150], Step [25/320], Loss: 0.0039
2024-02-10 11:10:35 | Epoch [7/150], Step [50/320], Loss: 0.0043
2024-02-10 11:11:17 | Epoch [7/150], Step [75/320], Loss: 0.0042
2024-02-10 11:11:58 | Epoch [7/150], Step [100/320], Loss: 0.0043
2024-02-10 11:12:40 | Epoch [7/150], Step [125/320], Loss: 0.0031
2024-02-10 11:13:22 | Epoch [7/150], Step [150/320], Loss: 0.0071
2024-02-10 11:14:03 | Epoch [7/150], Step [175/320], Loss: 0.0029
2024-02-10 11:14:45 | Epoch [7/150], Step [200/320], Loss: 0.0036
2024-02-10 11:15:27 | Epoch [7/150], Step [225/320], Loss: 0.0039
2024-02-10 11:16:08 | Epoch [7/150], Step [250/320], Loss: 0.0047
2024-02-10 11:16:50 | Epoch [7/150], Step [275/320], Loss: 0.0032
2024-02-10 11:17:32 | Epoch [7/150], Step [300/320], Loss: 0.0027
2024-02-10 11:18:50 | Epoch [8/150], Step [25/320], Loss: 0.003
2024-02-10 11:19:32 | Epoch [8/150], Step [50/320], Loss: 0.0033
2024-02-10 11:20:13 | Epoch [8/150], Step [75/320], Loss: 0.0027
2024-02-10 11:20:55 | Epoch [8/150], Step [100/320], Loss: 0.003
2024-02-10 11:21:37 | Epoch [8/150], Step [125/320], Loss: 0.0048
2024-02-10 11:22:18 | Epoch [8/150], Step [150/320], Loss: 0.0021
2024-02-10 11:23:00 | Epoch [8/150], Step [175/320], Loss: 0.0026
2024-02-10 11:23:42 | Epoch [8/150], Step [200/320], Loss: 0.0026
2024-02-10 11:24:23 | Epoch [8/150], Step [225/320], Loss: 0.0023
2024-02-10 11:25:05 | Epoch [8/150], Step [250/320], Loss: 0.0021
2024-02-10 11:25:46 | Epoch [8/150], Step [275/320], Loss: 0.0031
2024-02-10 11:26:28 | Epoch [8/150], Step [300/320], Loss: 0.0021
2024-02-10 11:27:47 | Epoch [9/150], Step [25/320], Loss: 0.0018
2024-02-10 11:28:28 | Epoch [9/150], Step [50/320], Loss: 0.0018
2024-02-10 11:29:10 | Epoch [9/150], Step [75/320], Loss: 0.002
2024-02-10 11:29:52 | Epoch [9/150], Step [100/320], Loss: 0.0021
2024-02-10 11:30:33 | Epoch [9/150], Step [125/320], Loss: 0.0024
2024-02-10 11:31:15 | Epoch [9/150], Step [150/320], Loss: 0.0017
2024-02-10 11:31:57 | Epoch [9/150], Step [175/320], Loss: 0.0024
2024-02-10 11:32:38 | Epoch [9/150], Step [200/320], Loss: 0.0014
2024-02-10 11:33:20 | Epoch [9/150], Step [225/320], Loss: 0.0013
2024-02-10 11:34:02 | Epoch [9/150], Step [250/320], Loss: 0.0014
2024-02-10 11:34:43 | Epoch [9/150], Step [275/320], Loss: 0.0013
2024-02-10 11:35:25 | Epoch [9/150], Step [300/320], Loss: 0.0028
2024-02-10 11:36:43 | Epoch [10/150], Step [25/320], Loss: 0.0017
2024-02-10 11:37:25 | Epoch [10/150], Step [50/320], Loss: 0.0016
2024-02-10 11:38:07 | Epoch [10/150], Step [75/320], Loss: 0.0027
2024-02-10 11:38:48 | Epoch [10/150], Step [100/320], Loss: 0.0018
2024-02-10 11:39:30 | Epoch [10/150], Step [125/320], Loss: 0.0012
2024-02-10 11:40:12 | Epoch [10/150], Step [150/320], Loss: 0.0017
2024-02-10 11:40:53 | Epoch [10/150], Step [175/320], Loss: 0.0018
2024-02-10 11:41:35 | Epoch [10/150], Step [200/320], Loss: 0.0012
2024-02-10 11:42:16 | Epoch [10/150], Step [225/320], Loss: 0.0013
2024-02-10 11:42:58 | Epoch [10/150], Step [250/320], Loss: 0.0018
2024-02-10 11:43:40 | Epoch [10/150], Step [275/320], Loss: 0.0013
2024-02-10 11:44:21 | Epoch [10/150], Step [300/320], Loss: 0.0016
  0%|                                                                                                    | 0/40 [00:00<?, ?it/s]








































100%|███████████████████████████████████████████████████████████████████████████████████████████| 40/40 [05:32<00:00,  8.31s/it]
current valid Dice: 0.0500
Best performance at epoch: 10, 0.0000 -> 0.0500
Save model in save_dir
2024-02-10 11:51:13 | Epoch [11/150], Step [25/320], Loss: 0.0011
2024-02-10 11:51:54 | Epoch [11/150], Step [50/320], Loss: 0.0012
2024-02-10 11:52:36 | Epoch [11/150], Step [75/320], Loss: 0.001
2024-02-10 11:53:18 | Epoch [11/150], Step [100/320], Loss: 0.0013
2024-02-10 11:53:59 | Epoch [11/150], Step [125/320], Loss: 0.0013
2024-02-10 11:54:41 | Epoch [11/150], Step [150/320], Loss: 0.001
2024-02-10 11:55:22 | Epoch [11/150], Step [175/320], Loss: 0.0012
2024-02-10 11:56:04 | Epoch [11/150], Step [200/320], Loss: 0.0011
2024-02-10 11:56:46 | Epoch [11/150], Step [225/320], Loss: 0.0012
2024-02-10 11:57:27 | Epoch [11/150], Step [250/320], Loss: 0.0009
2024-02-10 11:58:09 | Epoch [11/150], Step [275/320], Loss: 0.0011
2024-02-10 11:58:50 | Epoch [11/150], Step [300/320], Loss: 0.001
2024-02-10 12:00:09 | Epoch [12/150], Step [25/320], Loss: 0.001
2024-02-10 12:00:51 | Epoch [12/150], Step [50/320], Loss: 0.001
2024-02-10 12:01:32 | Epoch [12/150], Step [75/320], Loss: 0.0011
2024-02-10 12:02:14 | Epoch [12/150], Step [100/320], Loss: 0.001
2024-02-10 12:02:56 | Epoch [12/150], Step [125/320], Loss: 0.0008
2024-02-10 12:03:37 | Epoch [12/150], Step [150/320], Loss: 0.001
2024-02-10 12:04:19 | Epoch [12/150], Step [175/320], Loss: 0.0011
2024-02-10 12:05:00 | Epoch [12/150], Step [200/320], Loss: 0.001
2024-02-10 12:05:42 | Epoch [12/150], Step [225/320], Loss: 0.0008
2024-02-10 12:06:24 | Epoch [12/150], Step [250/320], Loss: 0.001
2024-02-10 12:07:05 | Epoch [12/150], Step [275/320], Loss: 0.0008
2024-02-10 12:07:47 | Epoch [12/150], Step [300/320], Loss: 0.0011
2024-02-10 12:09:06 | Epoch [13/150], Step [25/320], Loss: 0.0011
2024-02-10 12:09:47 | Epoch [13/150], Step [50/320], Loss: 0.0014
2024-02-10 12:10:29 | Epoch [13/150], Step [75/320], Loss: 0.001
2024-02-10 12:11:10 | Epoch [13/150], Step [100/320], Loss: 0.0008
2024-02-10 12:11:52 | Epoch [13/150], Step [125/320], Loss: 0.001
2024-02-10 12:12:34 | Epoch [13/150], Step [150/320], Loss: 0.0007
