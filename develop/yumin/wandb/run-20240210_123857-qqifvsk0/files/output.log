/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
2024-02-10 12:40:21 | Epoch [1/150], Step [25/160], Loss: 0.2007
2024-02-10 12:41:17 | Epoch [1/150], Step [50/160], Loss: 0.0685
2024-02-10 12:42:13 | Epoch [1/150], Step [75/160], Loss: 0.0417
2024-02-10 12:43:08 | Epoch [1/150], Step [100/160], Loss: 0.0318
2024-02-10 12:44:04 | Epoch [1/150], Step [125/160], Loss: 0.0266
2024-02-10 12:45:00 | Epoch [1/150], Step [150/160], Loss: 0.0238
2024-02-10 12:46:27 | Epoch [2/150], Step [25/160], Loss: 0.0203
2024-02-10 12:47:23 | Epoch [2/150], Step [50/160], Loss: 0.0171
2024-02-10 12:48:19 | Epoch [2/150], Step [75/160], Loss: 0.0165
2024-02-10 12:49:14 | Epoch [2/150], Step [100/160], Loss: 0.0133
2024-02-10 12:50:10 | Epoch [2/150], Step [125/160], Loss: 0.0129
2024-02-10 12:51:06 | Epoch [2/150], Step [150/160], Loss: 0.011
2024-02-10 12:52:33 | Epoch [3/150], Step [25/160], Loss: 0.0095
2024-02-10 12:53:29 | Epoch [3/150], Step [50/160], Loss: 0.0093
2024-02-10 12:54:25 | Epoch [3/150], Step [75/160], Loss: 0.0088
2024-02-10 12:55:21 | Epoch [3/150], Step [100/160], Loss: 0.0076
2024-02-10 12:56:17 | Epoch [3/150], Step [125/160], Loss: 0.0082
2024-02-10 12:57:13 | Epoch [3/150], Step [150/160], Loss: 0.0074
2024-02-10 12:58:39 | Epoch [4/150], Step [25/160], Loss: 0.0062
2024-02-10 12:59:35 | Epoch [4/150], Step [50/160], Loss: 0.0066
2024-02-10 13:00:31 | Epoch [4/150], Step [75/160], Loss: 0.0093
2024-02-10 13:01:27 | Epoch [4/150], Step [100/160], Loss: 0.007
2024-02-10 13:02:23 | Epoch [4/150], Step [125/160], Loss: 0.0059
2024-02-10 13:03:19 | Epoch [4/150], Step [150/160], Loss: 0.0068
2024-02-10 13:04:45 | Epoch [5/150], Step [25/160], Loss: 0.0062
2024-02-10 13:05:41 | Epoch [5/150], Step [50/160], Loss: 0.0059
2024-02-10 13:06:37 | Epoch [5/150], Step [75/160], Loss: 0.0063
2024-02-10 13:07:33 | Epoch [5/150], Step [100/160], Loss: 0.0062
2024-02-10 13:08:29 | Epoch [5/150], Step [125/160], Loss: 0.006
2024-02-10 13:09:25 | Epoch [5/150], Step [150/160], Loss: 0.0063
Start validation # 5















































































 99%|█████████████████████████████████████████████████████████████████████████████████████████████████▊ | 79/80 [05:54<00:04,  4.51s/it]
current valid Dice: 0.5420
Best performance at epoch: 5, 0.0000 -> 0.5420

100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [05:58<00:00,  4.49s/it]
2024-02-10 13:16:52 | Epoch [6/150], Step [25/160], Loss: 0.0059
2024-02-10 13:17:48 | Epoch [6/150], Step [50/160], Loss: 0.0055
2024-02-10 13:18:43 | Epoch [6/150], Step [75/160], Loss: 0.0058
2024-02-10 13:19:39 | Epoch [6/150], Step [100/160], Loss: 0.0056
2024-02-10 13:20:35 | Epoch [6/150], Step [125/160], Loss: 0.0084
2024-02-10 13:21:31 | Epoch [6/150], Step [150/160], Loss: 0.0054
2024-02-10 13:22:57 | Epoch [7/150], Step [25/160], Loss: 0.0055
2024-02-10 13:23:53 | Epoch [7/150], Step [50/160], Loss: 0.0046
2024-02-10 13:24:49 | Epoch [7/150], Step [75/160], Loss: 0.0045
2024-02-10 13:25:45 | Epoch [7/150], Step [100/160], Loss: 0.0044
2024-02-10 13:26:41 | Epoch [7/150], Step [125/160], Loss: 0.0041
2024-02-10 13:27:37 | Epoch [7/150], Step [150/160], Loss: 0.0038
2024-02-10 13:29:03 | Epoch [8/150], Step [25/160], Loss: 0.0037
2024-02-10 13:29:59 | Epoch [8/150], Step [50/160], Loss: 0.0033
2024-02-10 13:30:55 | Epoch [8/150], Step [75/160], Loss: 0.0027
2024-02-10 13:31:51 | Epoch [8/150], Step [100/160], Loss: 0.0023
2024-02-10 13:32:47 | Epoch [8/150], Step [125/160], Loss: 0.0024
2024-02-10 13:33:43 | Epoch [8/150], Step [150/160], Loss: 0.0023
2024-02-10 13:35:09 | Epoch [9/150], Step [25/160], Loss: 0.0021
2024-02-10 13:36:05 | Epoch [9/150], Step [50/160], Loss: 0.0024
2024-02-10 13:37:00 | Epoch [9/150], Step [75/160], Loss: 0.005
2024-02-10 13:37:56 | Epoch [9/150], Step [100/160], Loss: 0.0039
2024-02-10 13:38:52 | Epoch [9/150], Step [125/160], Loss: 0.0026
2024-02-10 13:39:48 | Epoch [9/150], Step [150/160], Loss: 0.0025
2024-02-10 13:41:14 | Epoch [10/150], Step [25/160], Loss: 0.0019
2024-02-10 13:42:10 | Epoch [10/150], Step [50/160], Loss: 0.0016
2024-02-10 13:43:06 | Epoch [10/150], Step [75/160], Loss: 0.0019
2024-02-10 13:44:02 | Epoch [10/150], Step [100/160], Loss: 0.0013
2024-02-10 13:44:58 | Epoch [10/150], Step [125/160], Loss: 0.0017
2024-02-10 13:45:53 | Epoch [10/150], Step [150/160], Loss: 0.0013
Start validation #10















































































 99%|█████████████████████████████████████████████████████████████████████████████████████████████████▊ | 79/80 [05:55<00:04,  4.49s/it]
current valid Dice: 0.9214
Best performance at epoch: 10, 0.5420 -> 0.9214

100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [05:59<00:00,  4.50s/it]
2024-02-10 13:53:20 | Epoch [11/150], Step [25/160], Loss: 0.0013
2024-02-10 13:54:16 | Epoch [11/150], Step [50/160], Loss: 0.0012
2024-02-10 13:55:12 | Epoch [11/150], Step [75/160], Loss: 0.0013
2024-02-10 13:56:08 | Epoch [11/150], Step [100/160], Loss: 0.0014
2024-02-10 13:57:04 | Epoch [11/150], Step [125/160], Loss: 0.0012
2024-02-10 13:58:00 | Epoch [11/150], Step [150/160], Loss: 0.0011
2024-02-10 13:59:26 | Epoch [12/150], Step [25/160], Loss: 0.0013
2024-02-10 14:00:22 | Epoch [12/150], Step [50/160], Loss: 0.001
2024-02-10 14:01:17 | Epoch [12/150], Step [75/160], Loss: 0.001
2024-02-10 14:02:13 | Epoch [12/150], Step [100/160], Loss: 0.0011
2024-02-10 14:03:09 | Epoch [12/150], Step [125/160], Loss: 0.001
2024-02-10 14:04:05 | Epoch [12/150], Step [150/160], Loss: 0.001
2024-02-10 14:05:31 | Epoch [13/150], Step [25/160], Loss: 0.0009
2024-02-10 14:06:27 | Epoch [13/150], Step [50/160], Loss: 0.001
2024-02-10 14:07:23 | Epoch [13/150], Step [75/160], Loss: 0.001
2024-02-10 14:08:19 | Epoch [13/150], Step [100/160], Loss: 0.001
2024-02-10 14:09:15 | Epoch [13/150], Step [125/160], Loss: 0.0011
2024-02-10 14:10:11 | Epoch [13/150], Step [150/160], Loss: 0.001
2024-02-10 14:11:37 | Epoch [14/150], Step [25/160], Loss: 0.0009
2024-02-10 14:12:33 | Epoch [14/150], Step [50/160], Loss: 0.0009
2024-02-10 14:13:29 | Epoch [14/150], Step [75/160], Loss: 0.001
2024-02-10 14:14:25 | Epoch [14/150], Step [100/160], Loss: 0.001
2024-02-10 14:15:21 | Epoch [14/150], Step [125/160], Loss: 0.0009
2024-02-10 14:16:17 | Epoch [14/150], Step [150/160], Loss: 0.0009
2024-02-10 14:17:43 | Epoch [15/150], Step [25/160], Loss: 0.0009
2024-02-10 14:18:39 | Epoch [15/150], Step [50/160], Loss: 0.001
2024-02-10 14:19:35 | Epoch [15/150], Step [75/160], Loss: 0.0009
2024-02-10 14:20:31 | Epoch [15/150], Step [100/160], Loss: 0.0009
2024-02-10 14:21:27 | Epoch [15/150], Step [125/160], Loss: 0.001
2024-02-10 14:22:23 | Epoch [15/150], Step [150/160], Loss: 0.0009
  0%|                                                                                                            | 0/80 [00:00<?, ?it/s]
















































































100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [05:58<00:00,  4.48s/it]
current valid Dice: 0.9348
Best performance at epoch: 15, 0.9214 -> 0.9348
Save model in save_dir
2024-02-10 14:29:49 | Epoch [16/150], Step [25/160], Loss: 0.0009
2024-02-10 14:30:45 | Epoch [16/150], Step [50/160], Loss: 0.0009
2024-02-10 14:31:41 | Epoch [16/150], Step [75/160], Loss: 0.0009
2024-02-10 14:32:37 | Epoch [16/150], Step [100/160], Loss: 0.0009
2024-02-10 14:33:33 | Epoch [16/150], Step [125/160], Loss: 0.0009
2024-02-10 14:34:28 | Epoch [16/150], Step [150/160], Loss: 0.0008
2024-02-10 14:35:55 | Epoch [17/150], Step [25/160], Loss: 0.0009
2024-02-10 14:36:51 | Epoch [17/150], Step [50/160], Loss: 0.001
2024-02-10 14:37:47 | Epoch [17/150], Step [75/160], Loss: 0.0009
2024-02-10 14:38:43 | Epoch [17/150], Step [100/160], Loss: 0.0009
2024-02-10 14:39:39 | Epoch [17/150], Step [125/160], Loss: 0.0009
2024-02-10 14:40:35 | Epoch [17/150], Step [150/160], Loss: 0.0008
2024-02-10 14:42:01 | Epoch [18/150], Step [25/160], Loss: 0.0008
2024-02-10 14:42:57 | Epoch [18/150], Step [50/160], Loss: 0.0009
2024-02-10 14:43:53 | Epoch [18/150], Step [75/160], Loss: 0.0008
2024-02-10 14:44:49 | Epoch [18/150], Step [100/160], Loss: 0.001
2024-02-10 14:45:44 | Epoch [18/150], Step [125/160], Loss: 0.0008
2024-02-10 14:46:40 | Epoch [18/150], Step [150/160], Loss: 0.0009
2024-02-10 14:48:07 | Epoch [19/150], Step [25/160], Loss: 0.0008
2024-02-10 14:49:03 | Epoch [19/150], Step [50/160], Loss: 0.001
2024-02-10 14:49:59 | Epoch [19/150], Step [75/160], Loss: 0.0008
2024-02-10 14:50:55 | Epoch [19/150], Step [100/160], Loss: 0.0009
2024-02-10 14:51:51 | Epoch [19/150], Step [125/160], Loss: 0.0007
2024-02-10 14:52:47 | Epoch [19/150], Step [150/160], Loss: 0.001
2024-02-10 14:54:13 | Epoch [20/150], Step [25/160], Loss: 0.0009
2024-02-10 14:55:09 | Epoch [20/150], Step [50/160], Loss: 0.0007
2024-02-10 14:56:05 | Epoch [20/150], Step [75/160], Loss: 0.0009
2024-02-10 14:57:01 | Epoch [20/150], Step [100/160], Loss: 0.0008
2024-02-10 14:57:57 | Epoch [20/150], Step [125/160], Loss: 0.0007
2024-02-10 14:58:53 | Epoch [20/150], Step [150/160], Loss: 0.0008
  0%|                                                                                                            | 0/80 [00:00<?, ?it/s]
















































































100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [06:06<00:00,  4.58s/it]
current valid Dice: 0.9361
Best performance at epoch: 20, 0.9348 -> 0.9361
Save model in save_dir
2024-02-10 15:06:27 | Epoch [21/150], Step [25/160], Loss: 0.0008
2024-02-10 15:07:23 | Epoch [21/150], Step [50/160], Loss: 0.0007
2024-02-10 15:08:19 | Epoch [21/150], Step [75/160], Loss: 0.0007
2024-02-10 15:09:15 | Epoch [21/150], Step [100/160], Loss: 0.0008
2024-02-10 15:10:11 | Epoch [21/150], Step [125/160], Loss: 0.0007
2024-02-10 15:11:07 | Epoch [21/150], Step [150/160], Loss: 0.0007
Traceback (most recent call last):
  File "/data/ephemeral/home/level2-cv-semanticsegmentation-cv-10/develop/yumin/custom.py", line 361, in <module>
    train(model, train_loader, valid_loader, criterion, optimizer)
  File "/data/ephemeral/home/level2-cv-semanticsegmentation-cv-10/develop/yumin/custom.py", line 305, in train
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt